{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39743fd",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2022-02-18 00:07:34.615092: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a12db6",
   "metadata": {},
   "source": [
    "# data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395ce0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wh_ETL(dir_path, idx, mode='train', feature_range=(0, 1), pickle_save_dir='model_save/test1/pickles'):\n",
    "    # load from folder\n",
    "    whs = list(Path(dir_path).glob('*.npy'))\n",
    "    whs = np.stack([np.load(path) for path in whs])\n",
    "    print(whs.shape)\n",
    "    nb_test_samples = int(0.2 * whs.shape[0])\n",
    "    whs = whs[idx]\n",
    "    if mode is 'train':\n",
    "        whs = whs[2*nb_test_samples:]\n",
    "    elif mode is 'val':\n",
    "        whs = whs[0:nb_test_samples]\n",
    "    else:\n",
    "        whs = whs[nb_test_samples:2*nb_test_samples]    \n",
    "    \n",
    "    # move sample axis to the last, reshape, and remove the first layer\n",
    "    whs = np.moveaxis(whs, 1, 3).reshape(-1, 34)[:, 1:]\n",
    "    \n",
    "    # Min Max normalization\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    scaled_whs = scaler.fit_transform(whs)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=0.96, svd_solver='full')\n",
    "    out = pca.fit_transform(scaled_whs)\n",
    "    \n",
    "    # save scaler and pca as pickles\n",
    "    if mode is 'train':\n",
    "        Path(pickle_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        pickle.dump(scaler, open(f'{pickle_save_dir}/scaler.pkl', 'wb'))\n",
    "        pickle.dump(pca, open(f'{pickle_save_dir}/pca.pkl', 'wb'))\n",
    "    return out, pca\n",
    "\n",
    "\n",
    "def load_X(dir_path):\n",
    "    Vars = []\n",
    "    names = [p.name for p in Path(X_path).glob('*/')]\n",
    "    for name in names:\n",
    "        path = Path(X_path).joinpath(name)\n",
    "        data = np.stack([np.load(p) for p in path.glob('*.npy')])\n",
    "        if name == 'cape':\n",
    "            data = data[:, np.newaxis, ..., np.newaxis]\n",
    "            data = np.concatenate([data]*34, axis=1)\n",
    "        else:\n",
    "            data = data[..., np.newaxis]\n",
    "        Vars.append(data)\n",
    "    X = np.concatenate(Vars, axis=-1)[:, 1:, ...]\n",
    "    X = np.moveaxis(X, 1, -2)\n",
    "\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "\n",
    "def pad_boundary(arr, kernel_size):\n",
    "    '''\n",
    "    arr : input array\n",
    "    kernel_size : training sample size\n",
    "    '''\n",
    "\n",
    "    pad_size = int((kernel_size - 1) / 2)\n",
    "\n",
    "    if len(arr.shape) == 5:\n",
    "        pad_arr = np.pad(arr, pad_size, 'wrap')\n",
    "        pad_arr = pad_arr[pad_size:-pad_size, ..., pad_size:-pad_size, pad_size:-pad_size]\n",
    "    else:\n",
    "        raise ValueError('len(arr.shape) should be 5')\n",
    "    return pad_arr\n",
    "\n",
    "\n",
    "def X_ETL(path, idx, mode='train'):\n",
    "    # load_X and select train/val/test set due to lack of memory\n",
    "    X = load_X(path)\n",
    "    nb_test_samples = int(0.2 * X.shape[0])\n",
    "    \n",
    "    print(X.shape)\n",
    "    X = X[idx]\n",
    "    if mode is 'train':\n",
    "        X = X[2*nb_test_samples:]\n",
    "    elif mode is 'val':\n",
    "        X = X[0:nb_test_samples]\n",
    "    else:\n",
    "        X = X[nb_test_samples:2*nb_test_samples]\n",
    "    \n",
    "    #X = pad_boundary(X, 7)\n",
    "    #X = np.lib.stride_tricks.sliding_window_view(X, (7, 7), axis=(1, 2))\n",
    "    #X = np.moveaxis(X, 4, -1)\n",
    "    # return X.reshape(-1, 33, 7, 7, 9)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8307e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "seed = 777\n",
    "idx = np.arange(1314)\n",
    "random.seed(seed)\n",
    "random.shuffle(idx)\n",
    "nb_test_samples = int(0.2 * idx.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84a3ea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 34, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# y preprocess\n",
    "wh_path = 'data/target/wh/'\n",
    "pickle_save_dir='model_save/test1/pickles'\n",
    "mode = 'train'\n",
    "y, pca = wh_ETL(wh_path, idx, mode=mode, pickle_save_dir=pickle_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b83b8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pickle.load(open('model_save/test1/pickles/pca.pkl', 'rb'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bc96d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808960, 33)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.inverse_transform(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ff3434",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1314, 32, 32, 33, 9)\n"
    }
   ],
   "source": [
    "# X preprocess\n",
    "mode = 'test'\n",
    "X_path = 'data/vars/'\n",
    "X = X_ETL(X_path, idx, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['cape', 'mfd', 'mse', 'q', 't', 'u', 'v', 'w', 'z']"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "names = [p.name for p in Path(X_path).glob('*/')]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_test_ori.npy', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4686d146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((268288, 33, 7, 7, 9), (268288, 5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3606970",
   "metadata": {},
   "source": [
    "# save as tf record dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "01ec76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    return array\n",
    "\n",
    "def parse_single_data(feature, label):\n",
    "    #define the dictionary -- the structure -- of our single example\n",
    "    data = {\n",
    "        'feature' : _bytes_feature(tf.io.serialize_tensor(feature).numpy()),\n",
    "        'label' : _bytes_feature(tf.io.serialize_tensor(label).numpy())\n",
    "    }\n",
    "\n",
    "    out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "    return out\n",
    "\n",
    "def write_data_to_tfr_short(datas, labels, filename:str=\"data\"):\n",
    "    filename= filename+\".tfrecords\"\n",
    "    writer = tf.io.TFRecordWriter(filename) #create a writer that'll store our data to disk\n",
    "    count = 0\n",
    "\n",
    "    for index in range(len(datas)):\n",
    "\n",
    "        #get the data we want to write\n",
    "        current_data = datas[index]\n",
    "        current_label = labels[index]\n",
    "\n",
    "        out = parse_single_data(feature=current_data, label=current_label)\n",
    "        writer.write(out.SerializeToString())\n",
    "        count += 1\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Wrote {count} elements to TFRecord\")\n",
    "    return count\n",
    "\n",
    "def _parse_data_function(example_proto):\n",
    "    data_feature_description = {\n",
    "        'feature' : tf.io.FixedLenFeature([], tf.string),\n",
    "        'label' : tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    # Parse the input tf.train.Example proto using the dictionary above.\n",
    "    features = tf.io.parse_single_example(example_proto, data_feature_description)\n",
    "    features['feature'] = tf.io.parse_tensor(features['feature'], 'float')\n",
    "    features['label'] = tf.io.parse_tensor(features['label'], 'double')\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc0e7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to TFRecord\n",
    "# write_data_to_tfr_short(X, y, filename='test_dataset')\n",
    "\n",
    "# read TFRecord\n",
    "raw_dataset = tf.data.TFRecordDataset('test_dataset.tfrecords')\n",
    "parsed_dataset = raw_dataset.map(_parse_data_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c402b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0603465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n",
      "(33, 7, 7, 9) (5,)\n"
     ]
    }
   ],
   "source": [
    "for data in parsed_dataset.take(10):\n",
    "    print(data['feature'].shape, data['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e916799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d9d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}